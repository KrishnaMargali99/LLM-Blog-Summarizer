{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3895a425",
   "metadata": {},
   "source": [
    "## LLM-Powered Blog Summarizer: Exploring Cutting-Edge Techniques\n",
    "\n",
    "In today's world of too much online information, it's important to quickly understand long blog posts. That's where Language Model (LLM) technologies come in. They're changing how we read and shorten big amounts of text. This project, called \"LLM-Powered Blog Summarizer: Exploring Cutting-Edge Techniques,\" looks into these advanced models.\n",
    "\n",
    "We start by picking out useful information from long blog posts. Using top LLMs like BART, T5, and Pegasus, we show how they can summarize blogs accurately and efficiently.\n",
    "\n",
    "We use different LLMs, each with its own way of working, to make short summaries of long blogs. By using BART, T5, and Pegasus models, we want to see what works best and compare their results.\n",
    "\n",
    "Behind the scenes, we check how similar the summaries are to a reference using cosine similarity. This helps us see if the summarization techniques are working well.\n",
    "\n",
    "Come join us as we explore LLMs and see how they're changing how we summarize blogs. It's all about the latest techniques making blog reading easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8de167",
   "metadata": {},
   "source": [
    "## Task 1: Extracting Blog Content\n",
    "\n",
    "Our first task is to read the content of a blog. We have a file called \"blog.txt\" where the blog is stored. Our goal here is to open the file, read its contents, and store them in a variable so we can work with them later. Let's go ahead and do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,string \n",
    "file_path = 'blog.txt'\n",
    "\n",
    "#--- Read in blog.txt file ----\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    blog_content = file.read()\n",
    "\n",
    "# #--- Inspect Blog txt ---\n",
    "print(blog_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00644a",
   "metadata": {},
   "source": [
    "## Task 2: Cleaning Blog Text\n",
    "\n",
    "Great job on reading the blog content! Now, let's clean up the text to make it easier to work with. We'll convert all the text to lowercase so that we don't have to worry about capitalization. Then, we'll remove any extra spaces between paragraphs to make the text more uniform. This will help us analyze the blog more effectively. Let's clean up the text and get it ready for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25204bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRITE YOUR CODE FOR TASK 2 ---\n",
    "text = blog_content.lower()\n",
    "\n",
    "#--- Remove double line breaks ('\\n\\n') from the text and assign the cleaned text to the variable cleaned_text ---\n",
    "cleaned_text = text.replace('\\n\\n', '')\n",
    "\n",
    "#--- Inspect cleaned_text ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fb09b",
   "metadata": {},
   "source": [
    "## Task 3: Summarizing Blog Content\n",
    "\n",
    "Well done on cleaning up the blog text! Now, let's take it a step further and create a summary of the blog. We'll use a special tool called BART to help us with this task. First, we'll load the BART tokenizer and model. Then, we'll tokenize the cleaned text and feed it into the model to generate a summary. The summary will capture the main points of the blog in a concise form. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see what insights we can gather from the blog!\n",
    "\n",
    "## Cosine Similarity for Text Summarization Testing:\n",
    "\n",
    "Cosine similarity is a way to see how much alike two sets of things are. In text summarization, it's used to check how similar a summary made by a computer is to one made by a person. It looks at the direction of the sets, not how big they are. This method works well for understanding if two texts are similar in meaning. It doesn't matter how long the texts are. Using cosine similarity helps us measure how good a summarization model is at making summaries that match the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c6897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n",
    "pip install transformers\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the tokenizer and model from the 'facebook/bart-large-cnn' checkpoint\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = TFBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Example cleaned text to summarize\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "inputs = tokenizer(cleaned_text, max_length=1024, return_tensors='tf', truncation=True)\n",
    "\n",
    "# Generate summary using the BART model with specified parameters\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=300,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "summary_bart = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Export the generated summary to a text file\n",
    "with open(\"summary_bart.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(summary_bart)\n",
    "\n",
    "print(\"Summary generated and saved to summary_bart.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b002f",
   "metadata": {},
   "source": [
    "## Task 4: Alternative Blog Summarization with T5 Model\n",
    "\n",
    "Fantastic job on generating the blog summary using BART! Now, let's explore another approach to summarizing the blog content. We'll use a different tool called T5 to help us with this task. Similar to before, we'll load the T5 tokenizer and model. Then, we'll tokenize the cleaned text and feed it into the model to generate a summary. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see how it compares to the one we created earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "# Tokenize the input text with the prefix \"summarize:\"\n",
    "input_text = \"summarize: \" + cleaned_text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode and print the generated summary\n",
    "summary_t5 = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary_t5)\n",
    "\n",
    "# Export the generated summary to a text file\n",
    "output_filepath = \"summary_t5.txt\"\n",
    "with open(output_filepath, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(summary_t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1878f8",
   "metadata": {},
   "source": [
    "## Task 5: Employing Pegasus Model for Blog Summarization\n",
    "\n",
    "Well done on exploring multiple approaches to summarizing the blog content! Now, let's try yet another method using Pegasus. We'll load the Pegasus tokenizer and model to help us with this task. Following a similar process as before, we'll tokenize the cleaned text and feed it into the model to generate a summary. The summary will give us a condensed version of the blog's key points. Once generated, we'll save the summary to a file for future reference. Let's generate the summary and see how it differs from the previous ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
    "\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(cleaned_text, return_tensors='tf')\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode and print the generated summary\n",
    "summary_pegasus = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary_pegasus)\n",
    "\n",
    "# Export the generated summary to a text file\n",
    "output_filepath = \"summary_pegasus.txt\"\n",
    "with open(output_filepath, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(summary_pegasus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
